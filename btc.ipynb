{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitfarms At-home Task\n",
    "\n",
    "- Submit two .ipynb workbooks – one for each strategy.\n",
    "- Submit the cumulative returns for each strategy – as csv or json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib as ta\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset\n",
    "- Split each dataset into in-sample and out-of-sample using 1st Jan 2023 as the split point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "btc_hist = pd.read_csv('btc-hist.csv')\n",
    "\n",
    "# Convert the 'time' column to datetime format\n",
    "btc_hist['time'] = pd.to_datetime(btc_hist['time'])\n",
    "\n",
    "# Set the 'time' column as the index\n",
    "btc_hist = btc_hist.set_index('time')\n",
    "\n",
    "# Split the datasets into in-sample and out-of-sample data\n",
    "split_date =pd.to_datetime('2023-01-01')\n",
    "btc_in_sample = btc_hist[:split_date]\n",
    "btc_out_sample = btc_hist[split_date:]\n",
    "\n",
    "# Save the datasets to parquet files\n",
    "btc_in_sample.to_parquet('btc_in_sample.parquet')\n",
    "btc_out_sample.to_parquet('btc_out_sample.parquet')\n",
    "\n",
    "# Load the datasets from parquet files\n",
    "btc_in_sample = pd.read_parquet('btc_in_sample.parquet')\n",
    "btc_out_sample = pd.read_parquet('btc_out_sample.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Sample development and testing \n",
    "1. Create an array of lookback periods from 5 to 90 inclusive in increments of 5.\n",
    "2. For each lookback period, fit each of the following volatility indicators:\n",
    "- Bollinger Bands\n",
    "- Keltner Channel\n",
    "- Donchian Channel\n",
    "\n",
    "    <i>Note: Besides lookback period, you will need to provide other parameters that are unique to each indicator – use discretion but be sensible.</i>\n",
    "\n",
    "3. For each combination of indicator and lookback, calculate the following:\n",
    "- Distance from closing price to lower band.\n",
    "- Distance from closing price to upper band.\n",
    "- Distance between upper and lower band (channel breadth).\n",
    "\n",
    "    <i>Note: it your choice whether you use price or some transformation such as log price.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Bollinger Bands\n",
    "def bollinger_bands(df, lookback):\n",
    "    df['MA'] = df['close'].rolling(window=lookback).mean()\n",
    "    df['BB_Upper'] = df['MA'] + 2 * df['close'].rolling(window=lookback).std()\n",
    "    df['BB_Lower'] = df['MA'] - 2 * df['close'].rolling(window=lookback).std()\n",
    "    return df\n",
    "\n",
    "# Function to compute Keltner Channel\n",
    "def keltner_channel(df, lookback):\n",
    "    df['TR'] = ta.TRANGE(df['high'], df['low'], df['close'])\n",
    "    df['ATR'] = df['TR'].rolling(window=lookback).mean()\n",
    "    df['KC_Middle'] = df['close'].rolling(window=lookback).mean()\n",
    "    df['KC_Upper'] = df['KC_Middle'] + 2 * df['ATR']\n",
    "    df['KC_Lower'] = df['KC_Middle'] - 2 * df['ATR']\n",
    "    return df\n",
    "\n",
    "# Function to compute Donchian Channel\n",
    "def donchian_channel(df, lookback):\n",
    "    df['Don_Upper'] = df['high'].rolling(window=lookback).max()\n",
    "    df['Don_Lower'] = df['low'].rolling(window=lookback).min()\n",
    "    return df\n",
    "\n",
    "# Array of lookback periods\n",
    "lookback_periods = list(range(5, 91, 5))\n",
    "\n",
    "# List to store features\n",
    "features = []\n",
    "\n",
    "# Generate features for each lookback period\n",
    "for lookback in lookback_periods:\n",
    "    btc_features = bollinger_bands(btc_in_sample.copy(), lookback)\n",
    "    btc_features = keltner_channel(btc_features, lookback)\n",
    "    btc_features = donchian_channel(btc_features, lookback)\n",
    "    \n",
    "    btc_features['BB_Distance_Lower'] = (btc_features['close'] - btc_features['BB_Lower']) / btc_features['close']\n",
    "    btc_features['BB_Distance_Upper'] = (btc_features['BB_Upper'] - btc_features['close']) / btc_features['close']\n",
    "    btc_features['BB_Breadth'] = (btc_features['BB_Upper'] - btc_features['BB_Lower']) / btc_features['close']\n",
    "    \n",
    "    btc_features['KC_Distance_Lower'] = (btc_features['close'] - btc_features['KC_Lower']) / btc_features['close']\n",
    "    btc_features['KC_Distance_Upper'] = (btc_features['KC_Upper'] - btc_features['close']) / btc_features['close']\n",
    "    btc_features['KC_Breadth'] = (btc_features['KC_Upper'] - btc_features['KC_Lower']) / btc_features['close']\n",
    "    \n",
    "    btc_features['Don_Distance_Lower'] = (btc_features['close'] - btc_features['Don_Lower']) / btc_features['close']\n",
    "    btc_features['Don_Distance_Upper'] = (btc_features['Don_Upper'] - btc_features['close']) / btc_features['close']\n",
    "    btc_features['Don_Breadth'] = (btc_features['Don_Upper'] - btc_features['Don_Lower']) / btc_features['close']\n",
    "    \n",
    "    features.append((lookback, btc_features))\n",
    "\n",
    "# Save the features to csv files for future reference\n",
    "for lookback, btc_features in features:\n",
    "    btc_features.to_csv(f'btc_features_{lookback}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and Normalize \n",
    "4. From step 3, you will have generated a set of features. Scale and normalized these features so that they are bounded and stationary and that these conditions remain true even during intervals of elevated volatility. Perform other treatments as you see fit to enhance the signal fidelity of these features (i.e. ability to explain variations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale and normalize features\n",
    "def scale_features(df, feature_cols):\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    return df\n",
    "\n",
    "# List of feature columns\n",
    "feature_cols = [\n",
    "    'BB_Distance_Lower', 'BB_Distance_Upper', 'BB_Breadth',\n",
    "    'KC_Distance_Lower', 'KC_Distance_Upper', 'KC_Breadth',\n",
    "    'Don_Distance_Lower', 'Don_Distance_Upper', 'Don_Breadth'\n",
    "]\n",
    "\n",
    "# Scale and normalize the features for each lookback period\n",
    "scaled_features = []\n",
    "for lookback, btc_features, in features:\n",
    "    btc_features_scaled = scale_features(btc_features.copy(), feature_cols)\n",
    "    scaled_features.append((lookback, btc_features_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Trading Strategies using PCA and Random Forest\n",
    "5. Create two systematic trading strategies based on these features.\n",
    "- Using PCA\n",
    "- Using Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA-based trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create PCA-based trading strategy\n",
    "def pca_strategy(df, feature_cols, n_components=2):\n",
    "    # Fill in missing values with mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[feature_cols] = imputer.fit_transform(df[feature_cols])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_features = pca.fit_transform(df[feature_cols])\n",
    "    \n",
    "    # Simple threshold-based trading strategy\n",
    "    df['PCA_Signal'] = np.where(pca_features[:, 0] > 0, 1, -1)\n",
    "    df['PCA_Returns'] = df['close'].pct_change().shift(-1) * df['PCA_Signal']\n",
    "    df['Cumulative_PCA_Returns'] = (1 + df['PCA_Returns']).cumprod() - 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply PCA strategy to each lookback period\n",
    "pca_strategies = []\n",
    "for lookback, btc_features_scaled in scaled_features:\n",
    "    btc_pca = pca_strategy(btc_features_scaled.copy(), feature_cols)\n",
    "    pca_strategies.append((lookback, btc_pca))\n",
    "\n",
    "# Save cumulative returns for PCA strategy\n",
    "for lookback, btc_pca in pca_strategies:\n",
    "    btc_pca[['Cumulative_PCA_Returns']].to_csv(f'btc_pca_returns_{lookback}.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest based trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create Random Forest-based trading strategy\n",
    "def random_forest_strategy(df, feature_cols):\n",
    "    df['Target'] = np.where(df['close'].shift(-1) > df['close'], 1, 0)\n",
    "    X = df[feature_cols].values\n",
    "    y = df['Target'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    df['RF_Signal'] = rf.predict(X)\n",
    "    df['RF_Returns'] = df['close'].pct_change().shift(-1) * (df['RF_Signal'] * 2 - 1)\n",
    "    df['Cumulative_RF_Returns'] = (1 + df['RF_Returns']).cumprod() - 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply Random Forest strategy to each lookback period\n",
    "rf_strategies = []\n",
    "for lookback, btc_features_scaled in scaled_features:\n",
    "    btc_rf = random_forest_strategy(btc_features_scaled.copy(), feature_cols)\n",
    "    rf_strategies.append((lookback, btc_rf))\n",
    "\n",
    "# Save cumulative returns for Random Forest strategy\n",
    "for lookback, btc_rf in rf_strategies:\n",
    "    btc_rf[['Cumulative_RF_Returns']].to_csv(f'btc_rf_returns_{lookback}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Performance Metrics\n",
    "6. Measure the Average Win Rate (%), Number of Trades, Sharpe, Ulcer Performance Index and granular Profit Factor for each strategy. By ‘granular’ Profit Factor, you need to ensure that the returns used in that calculation are based on hourly mark to market as per the frequency of the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for performance metrics\n",
    "def calculate_performance_metrics(df, strategy_col):\n",
    "    # Calculate basic metrics\n",
    "    total_return = df[strategy_col].iloc[-1]\n",
    "    annualized_return = (1 + total_return) ** (252 / len(df)) - 1\n",
    "    annualized_volatility = df[strategy_col].pct_change().std() * np.sqrt(252)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility\n",
    "    \n",
    "    # Calculate drawdowns\n",
    "    df['Drawdown'] = df[strategy_col] - df[strategy_col].cummax()\n",
    "    max_drawdown = df['Drawdown'].min()\n",
    "\n",
    "    # Calculate Average Win Rate Percentage\n",
    "    df['Returns'] = df[strategy_col].pct_change()\n",
    "    df['Wins'] = df['Returns'] > 0\n",
    "    avg_win_rate = df['Wins'].mean() * 100\n",
    "    \n",
    "    # Calculate Number of Trades\n",
    "    number_of_trades = df['Returns'].count()\n",
    "\n",
    "    # Calculate Ulcer Performance Index\n",
    "    df['Drawdown_Pct'] = df['Drawdown'] / df[strategy_col].cummax()\n",
    "    ulcer_index = np.sqrt(np.mean(df['Drawdown_Pct'] ** 2))\n",
    "    ulcer_performance_index = annualized_return / ulcer_index if ulcer_index != 0 else np.nan\n",
    "\n",
    "    # Calculate Profit Factor\n",
    "    total_gains = df[df['Returns'] > 0]['Returns'].sum()\n",
    "    total_losses = abs(df[df['Returns'] < 0]['Returns'].sum())\n",
    "    profit_factor = total_gains / total_losses if total_losses != 0 else np.nan\n",
    "\n",
    "    return total_return, annualized_return, annualized_volatility, sharpe_ratio, max_drawdown, avg_win_rate, number_of_trades, ulcer_performance_index, profit_factor\n",
    "\n",
    "# Calculate performance metrics for each strategy\n",
    "pca_performance = []\n",
    "rf_performance = []\n",
    "\n",
    "for lookback, btc_pca in pca_strategies:\n",
    "    pca_metrics_btc = calculate_performance_metrics(btc_pca, 'Cumulative_PCA_Returns')\n",
    "    pca_performance.append((lookback, 'BTC', *pca_metrics_btc))\n",
    "    \n",
    "for lookback, btc_rf in rf_strategies:\n",
    "    rf_metrics_btc = calculate_performance_metrics(btc_rf, 'Cumulative_RF_Returns')\n",
    "    rf_performance.append((lookback, 'BTC', *rf_metrics_btc))\n",
    "\n",
    "# Save performance metrics to CSV\n",
    "pca_performance_df = pd.DataFrame(pca_performance, columns=['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                            'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                            'Profit_Factor'])\n",
    "rf_performance_df = pd.DataFrame(rf_performance, columns=['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                          'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                          'Profit_Factor'])\n",
    "\n",
    "pca_performance_df.to_csv('pca_performance.csv', index=False)\n",
    "rf_performance_df.to_csv('rf_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Permutation Test\n",
    "7. Perform a Monte Carlo Permutation Test (1000 repetitions) using the in-sample data to test each strategy. For each permutation run, calculate the Average Win Rate (%), Number of Trades, Sharpe, Ulcer Performance Index and granular Profit Factor. Average these measures across the 1000 repetitions and come these to your strategy’s performance on the actual in-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monte carlo\n",
    "def monte_carlo_permutation_test(df, strategy_col, n_iterations=1000):\n",
    "    metrics = []\n",
    "    for _ in range(n_iterations):\n",
    "        permuted_returns = df[strategy_col].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "        df['Permuted_Returns'] = permuted_returns.cumsum()\n",
    "        metrics.append(calculate_performance_metrics(df, 'Permuted_Returns'))\n",
    "    \n",
    "    return np.mean(metrics, axis=0), np.std(metrics, axis=0)\n",
    "\n",
    "# Perform Monte Carlo Permutation Test for each strategy\n",
    "pca_monte_carlo = []\n",
    "rf_monte_carlo = []\n",
    "for lookback, btc_pca in pca_strategies:\n",
    "    pca_metrics, pca_std = monte_carlo_permutation_test(btc_pca, 'Cumulative_PCA_Returns')\n",
    "    pca_monte_carlo.append((lookback, 'BTC', *pca_metrics, *pca_std))\n",
    "    \n",
    "for lookback, btc_rf in rf_strategies:\n",
    "    rf_metrics, rf_std = monte_carlo_permutation_test(btc_rf, 'Cumulative_RF_Returns')\n",
    "    rf_monte_carlo.append((lookback, 'BTC', *rf_metrics, *rf_std))\n",
    "\n",
    "# Save Monte Carlo results to CSV\n",
    "pca_monte_carlo_df = pd.DataFrame(pca_monte_carlo, columns=['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                            'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                            'Profit_Factor', 'Total_Return_Std', 'Annualized_Return_Std', 'Annualized_Volatility_Std',\n",
    "                                                            'Sharpe_Ratio_Std', 'Max_Drawdown_Std', 'Avg_Win_Rate_Std', 'Number_of_Trades_Std',\n",
    "                                                            'Ulcer_Performance_Index_Std', 'Profit_Factor_Std'])\n",
    "rf_monte_carlo_df = pd.DataFrame(rf_monte_carlo, columns = ['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                            'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                            'Profit_Factor', 'Total_Return_Std', 'Annualized_Return_Std', 'Annualized_Volatility_Std',\n",
    "                                                            'Sharpe_Ratio_Std', 'Max_Drawdown_Std', 'Avg_Win_Rate_Std', 'Number_of_Trades_Std',\n",
    "                                                            'Ulcer_Performance_Index_Std', 'Profit_Factor_Std'])\n",
    "\n",
    "pca_monte_carlo_df.to_csv('pca_monte_carlo.csv', index=False)\n",
    "rf_monte_carlo_df.to_csv('rf_monte_carlo.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Strategies on Out-of-Sample Data\n",
    "8. Run each strategy on the out of sample data and calculate the performance metrics. Perform the same Monte Carlo Permutation Test on the OOS data and come the performance of permuted runs against performance on actual OOS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and Random Forest strategies to out-of-sample data\n",
    "out_sample_pca_strategies = []\n",
    "out_sample_rf_strategies = []\n",
    "for lookback, _ in scaled_features:\n",
    "    btc_out_sample_features = pd.read_csv(f'btc_features_{lookback}.csv')\n",
    "    btc_out_sample_pca = pca_strategy(btc_out_sample_features.copy(), feature_cols)\n",
    "    out_sample_pca_strategies.append((lookback, btc_out_sample_pca))\n",
    "    \n",
    "    btc_out_sample_rf = random_forest_strategy(btc_out_sample_features.copy(), feature_cols)\n",
    "    out_sample_rf_strategies.append((lookback, btc_out_sample_rf))\n",
    "\n",
    "# Calculate performance metrics for out-of-sample data\n",
    "out_sample_pca_performance = []\n",
    "out_sample_rf_performance = []\n",
    "for lookback, btc_pca in out_sample_pca_strategies:\n",
    "    pca_metrics = calculate_performance_metrics(btc_pca, 'Cumulative_PCA_Returns')\n",
    "    out_sample_pca_performance.append((lookback, 'BTC', *pca_metrics))\n",
    "    \n",
    "for lookback, btc_rf in out_sample_rf_strategies:\n",
    "    rf_metrics = calculate_performance_metrics(btc_rf, 'Cumulative_RF_Returns')\n",
    "    out_sample_rf_performance.append((lookback, 'BTC', *rf_metrics))\n",
    "\n",
    "# Save out-of-sample performance metrics to CSV\n",
    "out_sample_pca_performance_df = pd.DataFrame(out_sample_pca_performance, columns =['Lookback', 'Asset', 'Total_Return', 'Annualized_Return',\n",
    "                                                                                   'Annualized_Volatility', 'Sharpe_Ratio', 'Max_Drawdown',\n",
    "                                                                                   'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                                                   'Profit_Factor'])\n",
    "out_sample_rf_performance_df = pd.DataFrame(out_sample_rf_performance, columns = ['Lookback', 'Asset', 'Total_Return', 'Annualized_Return',\n",
    "                                                                                  'Annualized_Volatility', 'Sharpe_Ratio', 'Max_Drawdown',\n",
    "                                                                                  'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                                                  'Profit_Factor'])\n",
    "\n",
    "out_sample_pca_performance_df.to_csv('out_sample_pca_performance.csv', index=False)\n",
    "out_sample_rf_performance_df.to_csv('out_sample_rf_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Monte Carlos on Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monte carlo\n",
    "def monte_carlo_permutation_test(df, strategy_col, n_iterations=1000):\n",
    "    metrics = []\n",
    "    for _ in range(n_iterations):\n",
    "        permuted_returns = df[strategy_col].sample(frac=1, replace=False).reset_index(drop=True)\n",
    "        df['Permuted_Returns'] = permuted_returns.cumsum()\n",
    "        metrics.append(calculate_performance_metrics(df, 'Permuted_Returns'))\n",
    "    \n",
    "    return np.mean(metrics, axis=0), np.std(metrics, axis=0)\n",
    "\n",
    "# Perform Monte Carlo Permutation Test for each strategy\n",
    "pca_monte_carlo = []\n",
    "rf_monte_carlo = []\n",
    "for lookback, btc_pca in pca_strategies:\n",
    "    pca_metrics, pca_std = monte_carlo_permutation_test(btc_pca, 'Cumulative_PCA_Returns')\n",
    "    pca_monte_carlo.append((lookback, 'BTC', *pca_metrics, *pca_std))\n",
    "    \n",
    "for lookback, btc_rf in rf_strategies:\n",
    "    rf_metrics, rf_std = monte_carlo_permutation_test(btc_rf, 'Cumulative_RF_Returns')\n",
    "    rf_monte_carlo.append((lookback, 'BTC', *rf_metrics, *rf_std))\n",
    "\n",
    "# Save Monte Carlo results to CSV\n",
    "pca_monte_carlo_dfo = pd.DataFrame(pca_monte_carlo, columns=['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                             'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                             'Profit_Factor', 'Total_Return_Std', 'Annualized_Return_Std', 'Annualized_Volatility_Std',\n",
    "                                                             'Sharpe_Ratio_Std', 'Max_Drawdown_Std', 'Avg_Win_Rate_Std', 'Number_of_Trades_Std',\n",
    "                                                             'Ulcer_Performance_Index_Std', 'Profit_Factor_Std'])\n",
    "rf_monte_carlo_dfo = pd.DataFrame(rf_monte_carlo, columns = ['Lookback', 'Asset', 'Total_Return', 'Annualized_Return', 'Annualized_Volatility',\n",
    "                                                             'Sharpe_Ratio', 'Max_Drawdown', 'Avg_Win_Rate', 'Number_of_Trades', 'Ulcer_Performance_Index',\n",
    "                                                             'Profit_Factor', 'Total_Return_Std', 'Annualized_Return_Std', 'Annualized_Volatility_Std',\n",
    "                                                             'Sharpe_Ratio_Std', 'Max_Drawdown_Std', 'Avg_Win_Rate_Std', 'Number_of_Trades_Std',\n",
    "                                                             'Ulcer_Performance_Index_Std', 'Profit_Factor_Std'])\n",
    "\n",
    "pca_monte_carlo_dfo.to_csv('pca_monte_carlo_o.csv', index=False)\n",
    "rf_monte_carlo_dfo.to_csv('rf_monte_carlo_o.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Meta-Labelling\n",
    "9. Apply meta-labelling to each strategy and demonstrate the extent to which the strategy’s performance is enhanced through that corrective exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-labelling: Use the results of the primary model as features for a secondary model\n",
    "def apply_meta_labelling(df, primary_strategy_col, feature_cols):\n",
    "    df['Primary_Signal'] = np.where(df[primary_strategy_col].shift(-1) > df[primary_strategy_col], 1, 0)\n",
    "    X = df[feature_cols + ['Primary_Signal']].values\n",
    "    y = np.where(df['close'].shift(-1) > df['close'], 1, 0)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    df['Meta_Signal'] = rf.predict(X)\n",
    "    df['Meta_Returns'] = df['close'].pct_change().shift(-1) * (df['Meta_Signal'] * 2 - 1)\n",
    "    df['Cumulative_Meta_Returns'] = (1 + df['Meta_Returns']).cumprod() - 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply meta-labelling to each strategy\n",
    "meta_labelled_strategies = []\n",
    "for lookback, btc_pca in pca_strategies:\n",
    "    btc_meta = apply_meta_labelling(btc_pca.copy(), 'Cumulative_PCA_Returns', feature_cols)\n",
    "    meta_labelled_strategies.append((lookback, btc_meta))\n",
    "\n",
    "for lookback, btc_rf in rf_strategies:\n",
    "    btc_meta = apply_meta_labelling(btc_rf.copy(), 'Cumulative_RF_Returns', feature_cols)\n",
    "    meta_labelled_strategies.append((lookback, btc_meta))\n",
    "\n",
    "# Save cumulative returns for meta-labelled strategy\n",
    "for lookback, btc_meta in meta_labelled_strategies:\n",
    "    btc_meta[['Cumulative_Meta_Returns']].to_csv(f'btc_meta_returns_{lookback}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare In-Sample to Out-of Sample and Meta-Labelled returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path\n",
    "directory = \"/Users/qboy/Downloads/bitf/\"\n",
    "# Use glob to match the pattern 'btc_pca_returns*'\n",
    "pattern = os.path.join(directory, \"btc_pca_returns*.csv\")\n",
    "# List to store individual dataframes\n",
    "dfs = []\n",
    "# Loop through the CSV files and append to the list\n",
    "for file in glob.glob(pattern):\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(os.path.join(directory, \"combined_btc_pca_returns.csv\"), index=False)\n",
    "\n",
    "# Use glob to match the pattern 'btc_rf_returns*'\n",
    "pattern = os.path.join(directory, \"btc_rf_returns*.csv\")\n",
    "# List to store individual dataframes\n",
    "dfs = []\n",
    "# Loop through the CSV files and append to the list\n",
    "for file in glob.glob(pattern):\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(os.path.join(directory, \"combined_btc_rf_returns.csv\"), index=False)\n",
    "\n",
    "# Use glob to match the pattern 'btc_meta_returns*'\n",
    "pattern = os.path.join(directory, \"btc_meta_returns*.csv\")\n",
    "# List to store individual dataframes\n",
    "dfs = []\n",
    "# Loop through the CSV files and append to the list\n",
    "for file in glob.glob(pattern):\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes in the list\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(os.path.join(directory, \"combined_btc_meta_returns.csv\"), index=False)\n",
    "\n",
    "# read the combined dataframes\n",
    "combined_pca = pd.read_csv('combined_btc_pca_returns.csv')\n",
    "combined_rf = pd.read_csv('combined_btc_rf_returns.csv')\n",
    "combined_meta = pd.read_csv('combined_btc_meta_returns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Return for In-Sample PCA Strategy is -0.9222181652809942%.\n",
      "Average Return for In-Sample RF Strategy is 1.559678924405005e+166%.\n",
      "Average Return for Out-of-Sample PCA Strategy is nan%.\n",
      "Average Return for Out-of-Sample RF Strategy is nan%.\n",
      "Average Return for Meta Label Strategy is 7.763465679934562e+166%.\n"
     ]
    }
   ],
   "source": [
    "# read out of sample data\n",
    "out_sample_pca_performance = pd.read_csv('out_sample_pca_performance.csv')\n",
    "out_sample_rf_performance = pd.read_csv('out_sample_rf_performance.csv')\n",
    "# compare the average return for each dataset \n",
    "print(f'Average Return for In-Sample PCA Strategy is {combined_pca.Cumulative_PCA_Returns.mean()}%.')\n",
    "print(f'Average Return for In-Sample RF Strategy is {combined_rf.Cumulative_RF_Returns.mean()}%.')\n",
    "print(f'Average Return for Out-of-Sample PCA Strategy is {out_sample_pca_performance.Total_Return.mean()}%.')\n",
    "print(f'Average Return for Out-of-Sample RF Strategy is {out_sample_rf_performance.Total_Return.mean()}%.')\n",
    "print(f'Average Return for Meta Label Strategy is {combined_meta.Cumulative_Meta_Returns.mean()}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect on Limitations and Biases\n",
    "10. Critically comment on the limitations and biases (both systematic and idiosyncratic) that pertain to each trading strategy – one that used PCA, and the other the used RF. Be as precise with your mathematical reasoning.\n",
    "11. Critically comment on the limitations and biases (both systematic and idiosyncratic) that pertain to the use of meta-labelling to improve system performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA-Based Strategy: Limitations and Biases\n",
    "\n",
    "#### **Systematic Limitations and Biases**\n",
    "\n",
    "1. **Linear Assumptions:**\n",
    "   - **Description:** Principal Component Analysis (PCA) assumes that the underlying data relationships are linear.\n",
    "   - **Mathematical Reasoning:** PCA identifies the directions (principal components) that maximize variance under the constraint of orthogonality and linearity. This assumption might lead to suboptimal feature extraction if the true relationships between variables are nonlinear.\n",
    "   - **Impact:** In financial time series, where nonlinear dynamics are common, PCA might overlook important interactions between features, leading to suboptimal trading signals.\n",
    "\n",
    "2. **Stationarity Requirements:**\n",
    "   - **Description:** PCA assumes that the input data is stationary (mean and variance are constant over time).\n",
    "   - **Mathematical Reasoning:** The covariance matrix, which PCA relies on, is a function of the means and variances of the input features. Non-stationary data may distort the principal components, leading to unstable and unreliable results.\n",
    "   - **Impact:** Financial markets are often non-stationary, with changing volatilities and trends. PCA might yield components that are only relevant for specific periods, leading to overfitting and poor out-of-sample performance.\n",
    "\n",
    "3. **Variance Maximization:**\n",
    "   - **Description:** PCA maximizes the variance captured by each principal component, implicitly assuming that higher variance equates to more important information.\n",
    "   - **Mathematical Reasoning:** The first principal component explains the most variance, but this doesn't necessarily correspond to the most predictive or relevant feature for trading.\n",
    "   - **Impact:** In trading, high variance might capture noise or market shocks rather than consistent signals. This bias towards variance can lead to strategies that are overly sensitive to volatile periods, resulting in erratic trading performance.\n",
    "\n",
    "#### **Idiosyncratic Limitations and Biases**\n",
    "\n",
    "1. **Sensitivity to Outliers:**\n",
    "   - **Description:** PCA is sensitive to outliers in the data.\n",
    "   - **Mathematical Reasoning:** Outliers can disproportionately affect the covariance matrix, distorting the principal components and leading to misleading conclusions about the data's structure.\n",
    "   - **Impact:** In financial markets, outliers often occur due to sudden news or market events. PCA-based strategies may react strongly to these outliers, generating signals that are not reflective of underlying market trends.\n",
    "\n",
    "2. **Fixed Components:**\n",
    "   - **Description:** Once determined, PCA components are fixed and do not adapt to changing market conditions.\n",
    "   - **Mathematical Reasoning:** PCA decomposes the data into a set of orthogonal vectors (components) based on historical data. These components do not change unless PCA is re-run.\n",
    "   - **Impact:** As market dynamics evolve, the relevance of these fixed components may diminish, leading to outdated signals and deteriorating strategy performance.\n",
    "\n",
    "### Random Forest-Based Strategy: Limitations and Biases\n",
    "\n",
    "#### **Systematic Limitations and Biases**\n",
    "\n",
    "1. **Overfitting:**\n",
    "   - **Description:** Random Forests can easily overfit to the in-sample data.\n",
    "   - **Mathematical Reasoning:** Random Forests are composed of many decision trees, each built on random subsets of features and data points. While this reduces variance, it can also lead to models that perfectly capture noise in the training data, especially if the number of trees is high or the depth of the trees is unrestricted.\n",
    "   - **Impact:** Overfitting results in a model that performs well in-sample but poorly out-of-sample, as it fails to generalize to new data. This is particularly problematic in financial markets where conditions change frequently.\n",
    "\n",
    "2. **Feature Importance Bias:**\n",
    "   - **Description:** Random Forests can be biased towards features with more variability or a larger range.\n",
    "   - **Mathematical Reasoning:** In Random Forests, features that split the data into more homogeneous groups are considered more important. However, features with greater variability or a wider range of values are more likely to create such splits, even if they are not the most predictive.\n",
    "   - **Impact:** This can lead to a model that overemphasizes certain features, potentially ignoring more subtle but significant signals. The strategy might thus focus on noise rather than on meaningful trends.\n",
    "\n",
    "3. **Non-Stationarity Issues:**\n",
    "   - **Description:** Random Forests do not explicitly handle non-stationarity in the data.\n",
    "   - **Mathematical Reasoning:** The model assumes that the relationships learned during training remain valid over time. However, in financial markets, these relationships can change due to shifts in market regimes, leading to a mismatch between training and live data.\n",
    "   - **Impact:** This can result in significant degradation of performance when applied to out-of-sample data, as the model may be unable to adapt to new market conditions.\n",
    "\n",
    "#### **Idiosyncratic Limitations and Biases**\n",
    "\n",
    "1. **Complexity and Interpretability:**\n",
    "   - **Description:** Random Forests, while powerful, are complex and difficult to interpret.\n",
    "   - **Mathematical Reasoning:** A Random Forest is an ensemble of decision trees, each making a prediction. The final prediction is an average or majority vote, making it challenging to understand the contribution of individual features or trees.\n",
    "   - **Impact:** This lack of transparency can be a drawback in trading, where understanding the rationale behind a model's decisions is crucial for risk management and strategy refinement.\n",
    "\n",
    "2. **Sensitivity to Training Data:**\n",
    "   - **Description:** The model's performance is highly sensitive to the quality and quantity of the training data.\n",
    "   - **Mathematical Reasoning:** Random Forests are data-hungry and perform best when provided with large, diverse datasets. If the training data is not representative of future market conditions, the model's predictions will be unreliable.\n",
    "   - **Impact:** This reliance on extensive historical data can be problematic in markets with limited data availability or where historical patterns do not repeat, leading to suboptimal trading decisions.\n",
    "\n",
    "### Meta-Labelling: Limitations and Biases\n",
    "\n",
    "#### **Systematic Limitations and Biases**\n",
    "\n",
    "1. **Increased Model Complexity:**\n",
    "   - **Description:** Meta-labelling adds an additional layer of complexity to the trading strategy.\n",
    "   - **Mathematical Reasoning:** Meta-labelling involves training a secondary model that takes the output of the primary model as input features. This increases the dimensionality and complexity of the model, potentially leading to overfitting, especially if the meta-model is not sufficiently regularized.\n",
    "   - **Impact:** Higher complexity can result in a strategy that is more prone to overfitting and harder to interpret. This may reduce the robustness of the strategy when applied to new data.\n",
    "\n",
    "2. **Dependency on Primary Model:**\n",
    "   - **Description:** The effectiveness of meta-labelling is heavily dependent on the quality of the primary model's predictions.\n",
    "   - **Mathematical Reasoning:** If the primary model's signals are weak or noisy, the meta-label will inherit and possibly amplify these issues. The meta-model may also become overfitted to the idiosyncrasies of the primary model, rather than providing independent, corrective insights.\n",
    "   - **Impact:** This dependency can limit the potential benefits of meta-labelling, as any biases or errors in the primary model are carried over to the secondary model, reducing overall strategy performance.\n",
    "\n",
    "3. **Lag in Signal Generation:**\n",
    "   - **Description:** Meta-labelling introduces a delay in the trading signal.\n",
    "   - **Mathematical Reasoning:** The meta-label is typically generated based on the output of the primary model, which means the signal is inherently delayed by at least one period (e.g., day, hour). This delay can reduce the strategy's responsiveness to rapid market changes.\n",
    "   - **Impact:** In fast-moving markets, this lag can lead to missed opportunities or increased slippage, diminishing the overall effectiveness of the strategy.\n",
    "\n",
    "#### **Idiosyncratic Limitations and Biases**\n",
    "\n",
    "1. **Data Snooping Bias:**\n",
    "   - **Description:** The process of optimizing the meta-label based on historical performance can introduce data snooping bias.\n",
    "   - **Mathematical Reasoning:** When the meta-model is trained on the same data used to develop and evaluate the primary model, there is a risk of over-optimizing based on past data, which may not be predictive of future performance.\n",
    "   - **Impact:** This bias can lead to overly optimistic performance estimates and strategies that fail to perform well in out-of-sample or live trading.\n",
    "\n",
    "2. **Difficulty in Implementation:**\n",
    "   - **Description:** Implementing meta-labelling in practice can be challenging, especially in real-time trading environments.\n",
    "   - **Mathematical Reasoning:** Meta-labelling requires the timely and accurate generation of primary model signals, as well as the ability to process these signals to generate meta-labels quickly enough to execute trades. This adds operational complexity and potential points of failure.\n",
    "   - **Impact:** Any delays or errors in this process can lead to suboptimal trade execution, reducing the overall effectiveness of the strategy. Additionally, the added complexity may increase transaction costs and reduce net profitability.\n",
    "\n",
    "---\n",
    "\n",
    "<i>In summary, both PCA-based and Random Forest-based strategies have their inherent limitations and biases, stemming from their underlying mathematical assumptions and sensitivities. Meta-labelling, while potentially beneficial in enhancing strategy performance, introduces its own set of challenges, particularly related to complexity and dependency on the primary model. Understanding these limitations is crucial for developing robust trading strategies and managing risks effectively.</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
